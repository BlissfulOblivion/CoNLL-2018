{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoNLL MAIN PROGRAM\n",
    "## Encoder-Decoder with Attention\n",
    "### Current archetype: GRU with Attention (not biRNN; no context)\n",
    "--------------------------\n",
    "#### Plan:\n",
    "1. Finish commenting -- **DONE**\n",
    "2. Edit evaluation functions -- **DONE**\n",
    "3. Test with current design (get dev data and test on dev data) -- **DONE**\n",
    "4. Incorporate tags as context\n",
    "5. Test with tags as context\n",
    "6. Convert GRU to LSTM\n",
    "7. Test with LSTM structure \n",
    "8. Edit for BiRNN structure (LSTM)\n",
    "9. Test with fully developed structure\n",
    "10. Discuss future possibilities\n",
    "--------------------------\n",
    "#### Questions:\n",
    "1. What does .view() do? **Answer**: resizes tensors (see section 2 of tests)\n",
    "2. How does super() work with pytorch? **Answer**: Basic inheritance\n",
    "3. What is squeeze/unsqueeze? **Answer**: Squeeze squishes any one-sized dims in a tensor, unsqueeze adds a 1-sized dim at a given position in the tensor\n",
    "4. What does .size() do? **Answer**: returns size of tensor\n",
    "5. What is topk? **Answer**: Returns top k largest values of tensor along a specified dim\n",
    "--------------------------\n",
    "#### Need help with spots:\n",
    "1. Line 38 in \"# Training loop definition\": How to resize tensors so they can be concatenated? Purpose: to concatenate context (morphological tags) tensor to decoder hidden state (this is where it is initiated) -- **FIXED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''      ANNOTATION KEY      '''\n",
    "# S/E = self explanatory\n",
    "# ??? = uncertain of meaning/purpose\n",
    "# TUOP = texual update of progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division # ???\n",
    "from io import open # For opening files\n",
    "import unicodedata # To convert to ASCII -- necessary?\n",
    "import string # ???\n",
    "import re # For normalizing text -- necessary?\n",
    "import random # For randomizing samples of data\n",
    "\n",
    "import torch # Imports pytorch\n",
    "import torch.nn as nn # Imports nn from pytorch\n",
    "from torch import optim # For optimization (SGD)\n",
    "import torch.nn.functional as F # For linear functions in neural nets\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Sets what processor to use (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up language classes for indexing characters\n",
    "\n",
    "# Start and end of word tokens\n",
    "SOW_token = 0\n",
    "EOW_token = 1\n",
    "\n",
    "# Class to set up lemmas and inflected words -- find new name maybe?\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name # S/E\n",
    "        # Creates an index for char --> index, char count, and index --> char\n",
    "        #   plus total number of unique chars\n",
    "        self.char2index = {}\n",
    "        self.char2count = {}\n",
    "        self.index2char = {0: \"SOW\", 1: \"EOW\"}\n",
    "        self.n_chars = 2  # Count SOW and EOW\n",
    "\n",
    "    # Adds all characters in word to Lang\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    # Adds char information to/creates indexes and counts for Lang\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up tag class\n",
    "# Close to Lang class\n",
    "\n",
    "class Tags:\n",
    "    def __init__(self, name): \n",
    "        self.name = name\n",
    "        self.tag2index = {}\n",
    "        self.tag2count = {}\n",
    "        self.index2tag = {}\n",
    "        self.n_tags = 0\n",
    "    \n",
    "    def addTags(self, tags):\n",
    "        for tag in tags:\n",
    "            self.addTag(tag)\n",
    "    \n",
    "    def addTag(self, tag):\n",
    "        if tag not in self.tag2index:\n",
    "            self.tag2index[tag] = self.n_tags\n",
    "            self.tag2count[tag] = 1\n",
    "            self.index2tag[self.n_tags] = tag\n",
    "            self.n_tags += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Is this necessary or will it mess up our program? '''\n",
    "#    Test this\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in language and setting\n",
    "# Outputs lemma Lang, inflected word Lang, and pairs plus tags\n",
    "\n",
    "def readLangs(lang, setting):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file based on input language and setting (low/med/high) and split into lines\n",
    "    lines = open('%s_%s.txt' % (lang, setting), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs of lemmas/words and tags; normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines] # Makes lemma/word pairs\n",
    "    tags = [l.split('\\t')[2].split(';') for l in lines] # Makes list of tags used for context in decoder\n",
    "    pairs_tags = list(zip(pairs,tags)) # Zips matching lemma/word pairs and tag set\n",
    "\n",
    "    # Make Lang instances\n",
    "    lemmas = Lang(\"lemmas\") # S/E\n",
    "    inflected_words = Lang(\"inflected words\") # S/E\n",
    "\n",
    "    return lemmas, inflected_words, pairs_tags # S/E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to find maximum length for a word to normalize input length\n",
    "#    runs through all lemma/word pairs to find longest string and returns the length of that string\n",
    "\n",
    "def findMax(pairs):\n",
    "    max_length = 0\n",
    "    for line in pairs:\n",
    "        if len(line[0][0]) > max_length:\n",
    "            max_length = len(line[0][0])\n",
    "        if len(line[0][1]) > max_length:\n",
    "            max_length =len(line[0][1])\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 100 lemma/word pairs\n",
      "Finding maximum string length...\n",
      "Maximum string length: 23\n",
      "Counting lemmas/words...\n",
      "Counted lemmas/words:\n",
      "lemmas 21\n",
      "inflected words 21\n",
      "(['pa', 'pa'], ['N', 'NOM', 'SG'])\n",
      "{'V': 0, '3': 1, 'PL': 2, 'IMP': 3, 'N': 4, 'GEN': 5, 'SG': 6, 'DEF': 7, 'DAT': 8, 'NOM': 9, '1': 10, 'FUT': 11, 'IND': 12, '2': 13, 'PST': 14, 'VOC': 15, 'SBJV': 16, 'AUTO': 17, 'COND': 18, 'PRS': 19, 'ADJ': 20, 'MASC': 21, 'REL': 22, '2:SG': 23}\n"
     ]
    }
   ],
   "source": [
    "# Sets up lemmas, words, lemma/word-pairs and tags pairs, and maximum string length\n",
    "\n",
    "def prepareData(lang, setting):\n",
    "    lemmas, inflected_words, pairs_tags = readLangs(lang, setting) # S/E\n",
    "    tagclass = Tags(\"tagclass\") # S/E\n",
    "    print(\"Read %s lemma/word pairs\" % len(pairs_tags)) # TUOP\n",
    "    print(\"Finding maximum string length...\") # TUOP\n",
    "    max_length = findMax(pairs_tags) # S/E\n",
    "    print(\"Maximum string length: %s\" % max_length) # TUOP\n",
    "    print(\"Counting lemmas/words...\") # TUOP\n",
    "    for pairtag in pairs_tags:\n",
    "        lemmas.addWord(pairtag[0][0]) # S/E\n",
    "        inflected_words.addWord(pairtag[0][1]) # S/E\n",
    "        tagclass.addTags(pairtag[1]) # S/E\n",
    "    print(\"Counted lemmas/words:\") # TUOP\n",
    "    print(lemmas.name, lemmas.n_chars) # TUOP\n",
    "    print(inflected_words.name, inflected_words.n_chars) # TOUP\n",
    "    return lemmas, inflected_words, pairs_tags, tagclass, max_length # S/E\n",
    "\n",
    "\n",
    "lemmas, inflected_words, pairs_tags, tagclass, max_length = prepareData('irish', 'low') # S/E\n",
    "print(random.choice(pairs_tags)) # TUOP\n",
    "print(tagclass.tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder RNN\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__() # S/E\n",
    "        self.hidden_size = hidden_size # Sets size of hidden layer\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size) # Sets embedding layer\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size) # Sets GRU (change to LSTM later)\n",
    "\n",
    "    def forward(self, input, hidden): # Computes forward propogation\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # Reshapes tensor\n",
    "        output = embedded # Sets output to embedded for clarity of objectives\n",
    "        # Runs output and hidden through GRU; redefines as selves for clarity (change to LSTM later)\n",
    "        output, hidden = self.gru(output, hidden) \n",
    "        return output, hidden # S/E\n",
    "\n",
    "    def initHidden(self): # Used to initiate hidden layer dims\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device) # Returns torch tensor of zeros in given dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder RNN with Attention\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__() # S/E\n",
    "        self.hidden_size = hidden_size # Sets hidden layer size\n",
    "        self.output_size = output_size # Sets output layer size\n",
    "        self.dropout_p = dropout_p # Sets dropout rate\n",
    "        self.max_length = max_length # Sets maximum string length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size) # Sets embedding layer\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length) # Sets attention mechanism (read more)\n",
    "        # Combines attention mecahanims parts?\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size) \n",
    "        self.dropout = nn.Dropout(self.dropout_p) # Implements dropout\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size) # Sets GRU (change to LSTM later)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size) # Sets linear output function\n",
    "\n",
    "    \n",
    "    def adjustHiddenSize(self, newsize): #? Maybe?\n",
    "        self.hidden_size = newsize\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs): # Forward propogation\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # Reshapes embedding tensor for hidden layer shape\n",
    "        embedded = self.dropout(embedded) # Implements dropout of embedded layer\n",
    "\n",
    "        # Sets attention weights (read more)\n",
    "        #    takes softmax of linear attention functions from concatenated embedding layer and hidden layer\n",
    "        #    What is the dim=1? and why 0 index? (figure out later)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # ??? Look this up again and try to figure it out (later)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # ??? Concatenates embedding tensors and attention, but why? and why 0 index?\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1) \n",
    "        output = self.attn_combine(output).unsqueeze(0) # Figure all this attention stuff out later\n",
    "\n",
    "        output = F.relu(output) # Runs output through ReLU function\n",
    "        output, hidden = self.gru(output, hidden) # Runs output and hidden state through GRU\n",
    "\n",
    "        # Runs output through linear function, then log softmax; why 0 index and why dim 1?\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1) \n",
    "        return output, hidden, attn_weights # S/E\n",
    "\n",
    "    def initHidden(self): # Used to initiate hidden layer dims\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device) # Returns torch tensor of zeros in given dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates tensors from indexes\n",
    "\n",
    "def indexesFromWord(Lang, word): # Accesses character indexes of input word from Lang\n",
    "    return [Lang.char2index[char] for char in word] # S/E\n",
    "\n",
    "\n",
    "def tensorFromWord(Lang, word): # Creates tensor from input word\n",
    "    indexes = indexesFromWord(Lang, word) # Gets list of indexes of characters in word\n",
    "    indexes.append(EOW_token) # Adds end of word token\n",
    "    # Returns tensor based on character indexes input\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1) # S/E\n",
    "\n",
    "def indexesFromTags(Tags, tags): # Accesses tag indexes of input tags from Tags\n",
    "    return [Tags.tag2index[tag] for tag in tags] # S/E\n",
    "\n",
    "def tensorFromTags(Tags, tags):\n",
    "    indexes = indexesFromTags(Tags, tags) # Gets list of indexes of characters in word\n",
    "    # Returns tensor based on character indexes input\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1,1, -1) # S/E\n",
    "\n",
    "def tensorsFromPair(pair): # Creates tensor from lemma/word pair\n",
    "    input_tensor = tensorFromWord(lemmas, pair[0][0]) # Creates input tensor\n",
    "    context_tensor = tensorFromTags(tagclass, pair[1]) # Creates context tensor\n",
    "    target_tensor = tensorFromWord(inflected_words, pair[0][1]) # Creates target output tensor\n",
    "    return (input_tensor, context_tensor, target_tensor) # S/E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop defintion\n",
    "\n",
    "teacher_forcing_ratio = 0.5 # Sets probability of teacher forcing occuring\n",
    "\n",
    "# Defines train function; following is each variable's function, in order:\n",
    "#    input_tensor = input tensor\n",
    "#    context_tensor = context tensor (from tags)\n",
    "#    target_tensor = target tensor\n",
    "#    encoder = instance of EncoderRNN\n",
    "#    decoder = instance of AttnDecoderRNN\n",
    "#    encoder_optimizer, decoder_optimizer = optimization algorithm (in this case SGD)\n",
    "#    max_length = maximum string length\n",
    "#    criterion = loss function (in this case negative log loss)\n",
    "def train(input_tensor, context_tensor, target_tensor, encoder, decoder, encoder_optimizer,\\\n",
    "          decoder_optimizer, max_length, criterion):\n",
    "    encoder_hidden = encoder.initHidden() # Returns hidden layer dim for encoder\n",
    "\n",
    "    encoder_optimizer.zero_grad() # Resets optimizer\n",
    "    decoder_optimizer.zero_grad() # Resets optimizer\n",
    "\n",
    "    input_length = input_tensor.size(0) # Input tensor length\n",
    "    target_length = target_tensor.size(0) # Target tensor length\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) # Sets encoder output dims\n",
    "\n",
    "    loss = 0 # Sets/resets loss to 0\n",
    "\n",
    "    for ei in range(input_length): # Tensor inputs\n",
    "        # Calculates encoder output and hidden state based on input tensor and encoder hidden state\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden) \n",
    "        encoder_outputs[ei] = encoder_output[0, 0] # Stores encoder outputs (figure out later why these indexes)\n",
    "\n",
    "    # Initiates decoder input with start of word token\n",
    "    decoder_input = torch.tensor([[SOW_token]], device=device) \n",
    "\n",
    "    decoder_hidden = encoder_hidden # Initiates decoder hidden state with final encoder hidden state\n",
    "    # Adds tags (context) to initial decoder hidden state\n",
    "    decoder_hidden = torch.cat((decoder_hidden, context_tensor.to(torch.float)), 2)\n",
    "    \n",
    "    new_hid_size = list(decoder_hidden.size())[2]\n",
    "    print(new_hid_size)\n",
    "    decoder.adjustHiddenSize(new_hid_size)\n",
    "\n",
    "    # Randomly decide when to or not to use teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False \n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length): # Iterates through target tensors\n",
    "            # Runs input, decoder hidden state, and encoder outputs through decoder\n",
    "            #    and sets decoder output, hidden state, and attention to output of decoder\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di]) # Calculates loss\n",
    "            # Teacher forcing: makes next input the target input instead of guessed output\n",
    "            decoder_input = torch.cat((target_tensor[di], context_tensor), 1)\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            # Runs input, decoder hidden state, and encoder outputs through decoder\n",
    "            #    and sets decoder output, hidden state, and attention to output of decoder\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1) # ??? Generator? but why topv?\n",
    "            # \"detach from history as input\" -- what does this mean/do?\n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di]) # Calculates loss\n",
    "            if decoder_input.item() == EOW_token: # S/E\n",
    "                break\n",
    "\n",
    "    loss.backward() # S/E\n",
    "\n",
    "    encoder_optimizer.step() # S/E\n",
    "    decoder_optimizer.step() # S/E\n",
    "\n",
    "    return loss.item() / target_length # S/E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates timer functions; Not totally relevant\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff for plotting; Mess with later\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines training iteration function\n",
    "\n",
    "# Defines training iteration function; explanation of each input vairable:\n",
    "#    encoder = instance of EncoderRNN\n",
    "#    decoder = instance of AttnDecoderRNN\n",
    "#    n_iters = number of iterations\n",
    "#    print_every=# = print TUOP every # iterations\n",
    "#    plot_every=# = plot TUOP every # iterations\n",
    "#    learning_rate=# = S/E\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time() # Starts timer\n",
    "    plot_losses = [] # ??? Probably for plotting -- deal with later\n",
    "    print_loss_total = 0  # Reset every print_every -- ??? I think: resets loss total to print\n",
    "    plot_loss_total = 0  # Reset every plot_every -- ??? I think: resets loss total to plot\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) # Sets encoder optimizer\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate) # Sets decoder optimizer\n",
    "    # Sets up training data of tensors from \n",
    "    #    randomized selections of lemma/word pairs for the number of iterations\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs_tags)) \n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss() # Sets loss function\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1] # Selects a training pair\n",
    "        input_tensor = training_pair[0] # Takes input string's tensor\n",
    "        context_tensor = training_pair[1] # Takes context tags' tensor\n",
    "        target_tensor = training_pair[2] # Take target string's tensor\n",
    "\n",
    "        loss = train(input_tensor, context_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, max_length, criterion) # S/E\n",
    "        print_loss_total += loss # Adds current loss to total loss for printing\n",
    "        plot_loss_total += loss # Adds current loss to toal loss for plotting\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every # Print avg of loss over iterations\n",
    "            print_loss_total = 0 # Resets total loss to print\n",
    "            # TUOP in terms of % complete and time taken, etc\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0: # Plot stuff; check out later\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses) # Shows plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation code\n",
    "\n",
    "# Defines evalutate:\n",
    "#    encoder, decoder = instances of EncoderRNN, AttnDecoderRNN\n",
    "#    lemma = word\n",
    "#    context = context tags\n",
    "#    max_length = maximum string length\n",
    "def evaluate(encoder, decoder, lemma, context, max_length):\n",
    "    with torch.no_grad(): # Keeps it from training\n",
    "        input_tensor = tensorFromWord(lemmas, lemma) # Creates input tensor\n",
    "        context_tesnor = tensorFromTags(tagclass, context) # Creates context tensor\n",
    "        input_length = input_tensor.size()[0] # S/E\n",
    "        encoder_hidden = encoder.initHidden() # Sets up hidden layer dims\n",
    "\n",
    "        # Sets up encoder outputs dims and sets them to zeros\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length): # Encoder input\n",
    "            # Runs input tensor and encoder hidden state through encoder\n",
    "            #    and sets encoder output and hidden state\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            # Adds encoder output to list of encoder outputs -- details later\n",
    "            encoder_outputs[ei] += encoder_output[0, 0] \n",
    "            \n",
    "        # Sets encoder input to tensor of start of word token\n",
    "        decoder_input = torch.tensor([[SOW_token]], device=device)  \n",
    "\n",
    "        decoder_hidden = encoder_hidden # Sets decoder hidden state to encoder hidden state\n",
    "        # Adds tags (context) to initial decoder hidden state\n",
    "        decoder_hidden = torch.cat((decoder_hidden, context_tensor), 1)\n",
    "\n",
    "        decoded_chars = [] # Initiates list of decoded words\n",
    "        # ??? Sets decoder attentions?\n",
    "        #    To tensor of zeros of maximum string length as dimensions? Why those dims?\n",
    "        #    Figure that out later\n",
    "        decoder_attentions = torch.zeros(max_length, max_length) \n",
    "\n",
    "        for di in range(max_length): # Decoder input\n",
    "            # Runs input tensor and decoder hidden state through decoder\n",
    "            #    and sets decoder output and hidden state\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data # ??? .data? Probably the generator\n",
    "            topv, topi = decoder_output.data.topk(1) # ??? topv, topi? .data.topk?\n",
    "            if topi.item() == EOW_token: # Breaks if top word is end of word token(? what is topi?)\n",
    "                decoded_chars.append('<EOW>')\n",
    "                break\n",
    "            else:\n",
    "                # Appends topi(?) word to decoded word list\n",
    "                decoded_chars.append(inflected_words.index2char[topi.item()]) \n",
    "                \n",
    "            decoder_input = topi.squeeze().detach() # Details later\n",
    "\n",
    "        return decoded_chars, decoder_attentions[:di + 1] # Returns decoded word and attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly evaluate words\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, pairs_tags, n=10): # n=# = number of samples to evaluate\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_tags)\n",
    "        print('>', pair[0][0])\n",
    "        print('=', pair[0][1])\n",
    "        output_chars, attentions = evaluate(encoder, decoder, pair[0][0], pair[1], max_length)\n",
    "        output_word = ''.join(output_chars)\n",
    "        print('<', output_word)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 516], m2: [512 x 23] at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/TH/generic/THTensorMath.c:2033",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-9d5a8a757d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Executes program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-24da4def4c7d>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         loss = train(input_tensor, context_tensor, target_tensor, encoder,\n\u001b[0;32m---> 31\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, max_length, criterion) # S/E\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;31m# Adds current loss to total loss for printing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;31m# Adds current loss to toal loss for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-7da6798a1bac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, context_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, max_length, criterion)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m#    and sets decoder output, hidden state, and attention to output of decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0;32m---> 64\u001b[0;31m                 decoder_input, decoder_hidden, encoder_outputs)\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ??? Ask Adam or Sarah later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# \"detach from history as input\" -- what does this mean/do?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-679d33d0277a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#    What is the 1 being concatenated? And why dim 1? (figure out later)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         attn_weights = F.softmax(\n\u001b[0;32m---> 31\u001b[0;31m             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# ??? Look this up again and try to figure it out (later)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 516], m2: [512 x 23] at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/TH/generic/THTensorMath.c:2033"
     ]
    }
   ],
   "source": [
    "hidden_size = 256 # Hidden layer size\n",
    "# Initiates instance of EncoderRNN with input size of number of unique chars? and hidden size as above\n",
    "encoder1 = EncoderRNN(lemmas.n_chars, hidden_size).to(device) \n",
    "# Initiates instance of AttnDecoderRNN with hidden size as above, output size of number of unique chars?\n",
    "#    the maximum string length, and the dropout rate\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, inflected_words.n_chars, max_length, dropout_p=0.1).to(device)\n",
    "\n",
    "# Executes program\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, pairs_tags) # Evaluates random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements evaluation of system\n",
    "\n",
    "output_chars, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"amhran\", max_length) # Outputs word and attentions from system given input word\n",
    "plt.matshow(attentions.numpy()) # Plots attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates plots of attentions and evaluations of system output\n",
    "# DO THIS LATER\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_lemmas, dev_inflected_words, dev_pairs_tags, dev_max_length = prepareData('irish', 'dev') # S/E\n",
    "print(random.choice(pairs_tags)) # TUOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1,dev_pairs_tags) # Evaluates random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST of readLangs\n",
    "# WORKS\n",
    "\n",
    "def readLangs(lang1, setting, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s_%s.txt' % (lang1, setting), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "    tags = [l.split('\\t')[2].split(';') for l in lines]\n",
    "    pairs_tags = list(zip(pairs,tags))\n",
    "\n",
    "    # Make Lang instances\n",
    "    lemmas = Lang(\"lemmas\")\n",
    "    inflected_words = Lang(\"inflected words\")\n",
    "\n",
    "    return lemmas, inflected_words, pairs_tags\n",
    "\n",
    "\n",
    "readLangs(\"irish\",\"low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.range(1, 16)\n",
    "print(a)\n",
    "print(a.size(0))\n",
    "a = a.view(4,4)\n",
    "print(a)\n",
    "print(a.size(1))\n",
    "print(\"\\n----\\n\")\n",
    "b = torch.range(1,27)\n",
    "print(b)\n",
    "print(b.size(0))\n",
    "b = b.view(3,3,3)\n",
    "print(\"1\", b)\n",
    "b = b.unsqueeze(3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"V;3;PL;IMP\"\n",
    "a = a.split(';')\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
